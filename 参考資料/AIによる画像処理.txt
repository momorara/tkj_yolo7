コンピュータの中では、画像ファィルはすべての情報を「ビット（0と1）」で扱っています。
画像という認識はなく、1次元の
11101100 10101000 00001111 00111010 ...
というデータです。
ファィルフォーマットにより意味づけは違いますが、RGBのデータが入っています。

添付の図はよくみると思います。
入力に猫の画像をビートデータとして入れ、これは猫:0 になるように内部構造を作るように指示します。
次に入力に犬の画像を入れ、これは犬:1になるように内部構造を作るように指示します。

これを何百回、何千回、と繰り返して、ニューラルネットワークの構造を作り、犬猫判定ができるようになります。
ニューラルネットワークは多層の非線形変換によって特徴を抽出するため、人間にはその判断基準が理解しづらく、
いわゆる「ブラックボックス」と呼ばれます。
入力のわずかな変化や学習データの偏りに敏感なため、「なぜ誤ったのか」を人間が明確に説明するのは難しいという意味で、「確率的にしか説明できない」と表現されることもあります。

図1

もうすこし、砕けたAIによる分類の説明として

学習（トレーニング）では：
AIは、多くの犬や猫の画像を見て、
それぞれの画像から「形・色・模様・耳の形・目の位置」など、
何次元もの特徴量を自動的に抽出します。
そして、
犬と猫の特徴が最もよく分かれるように、
AIの中の「重み（パラメータ）」を少しずつ調整していきます。
（イメージとしては、犬と猫を分ける多次元境界線（あるいは境界面）を作るようなものです。）

分類（予測）では：
新しい写真が与えられると、
AIは同じように特徴量を計算して、
その結果が境界面の「犬側」か「猫側」かを判定します。
つまり、
犬と猫の平均的な特徴を覚えておき、
それにどちらが近いかを判断しているような動きをします。